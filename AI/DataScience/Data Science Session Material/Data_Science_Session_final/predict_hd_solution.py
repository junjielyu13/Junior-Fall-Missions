# -*- coding: utf-8 -*-
"""predict_hd_solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RFgmEfaQxk7_rYevMF-nxLRv9BJ4qf6m

Heart disease prediction using Random Forests

This code is accompanied with several tips including classes, functions and methods to use. Please note that you do not have to follow these tips, but they might be handy in some cases.

Author: Polyxeni Gkontra (polyxeni.gkontra@ub.edy)
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, roc_curve
from sklearn.metrics import accuracy_score
from matplotlib import pyplot as plt

"""Read the .csv file with the patient information (TIP: you can use read_csv function from pandas) """

df = pd.read_csv('hd_data.csv')

"""Explore your data. E.g. Print the file or few lines to see how it looks like (TIP: If you want to see just few lines consider function head(). Functions dtypes() and describe() are useful to check the type of the data in each column and statistical properties, respectively)"""

df.head()

"""Check statistical details on your data like counts, min, max etc"""

df.describe()

"""Check the type of the features"""

df.dtypes

"""Check how many patients you have from each category

Split your dataset into training and testing creating a balanced dataset (TIP: 1. You can drop patients from the majority class, 2. You can use from scikit-learn, train_test_split(), 3. Good ML practices suggest to always shuffle your data, for dataframes you can use sample but watch out to add reset_index(drop=True) to reset the idnex but avoid creating a column with the index)
"""

# Drop subjects from majority class
label = 'HeartDisease'
healthy = df[df[label] == 0]
diseased = df[df[label] == 1]
print('Number of individuals from class 0 before balancing', healthy.shape[0])
print('Number of individuals from class 1 before balancing', diseased.shape[0])

# Delete the ones extra so that the number if equal
if healthy.shape[0] > diseased.shape[0]:
  healthy = healthy.iloc[0:diseased.shape[0],:]
elif healthy.shape[0] < diseased.shape[0]:
  diseased = diseased.iloc[0:healthy.shape[0],:]

print('Number of individuals of class 0 after balancing', healthy.shape[0])
print('Number of individuals of class 1 after balancing', diseased.shape[0])
# Concatenate
temp = [healthy, diseased]
df = pd.concat(temp)

# Shuffle the data 
df = df.sample(frac=1, random_state=1).reset_index(drop=True)

# Separate features from output & convert to numpy - You can also continue in pandas 
Y = df[label].copy()
X = df.drop(label, axis=1)

# Indices of training subjects and of testing
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=False, random_state=25)

"""Data pre-processing 

1.   Encode categorical variables  (TIP: Check OrdinalEncoder() from sklearn.preprocessing. Another popular approaches is OneHotEncoder but not appropriate for tree based classifiers, can you imagine why?)
2.   Scale numerical data (TIP: Check MinMaxScaler())

TIP: 1. It is very important to treat testing and training data separately to avoid data leakage 2. Methods fit_transform (for training data) and tranfrom (for the testing data) from can be very helpful. Alternatively you can use Pipeline and ColumnTransformer from sklearn but it will be more complicated to retrieve feature names for the most important features


"""

# Indices of categorical and numerical columns
# Categorical features
cat_feat = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina','ST_Slope']
# Indices of categorical features
categorical_idx = [loc for loc, key in enumerate(X_train.columns) if key in cat_feat]
# Indices of columns with numerical data
numerical_idx = list(set(range(0,X_train.shape[1])) - set(categorical_idx))
# Names of numerical features 
num_feat = X_train.columns[numerical_idx]

# Handle categorical variables
cat_preprocesssor = OrdinalEncoder()
X_train[cat_feat] = cat_preprocesssor.fit_transform(X_train[cat_feat])
X_test[cat_feat] = cat_preprocesssor.transform(X_test[cat_feat])
# Normalize the numerical valyes
num_preprocesssor = MinMaxScaler()
X_train[num_feat] = num_preprocesssor.fit_transform(X_train[num_feat])
X_test[num_feat] = num_preprocesssor.transform(X_test[num_feat])

"""Train the model (TIP: Check method fit)"""

# Random forest classifier with 500 trees, random_state is set to be able to reproduce your results
estimator = RandomForestClassifier(100, max_depth=10, random_state=42)
estimator.fit(X_train, Y_train)

"""Apply the model to the testing data and evaluate its perfromance (TIP: You might use classification_report or roc_auc_score)"""

# Make the prediction
Y_pred = estimator.predict(X_test)
# Evaluate the performance by comparing the predicted labels with the true ones
print(classification_report(Y_test,Y_pred))
accuracy = accuracy_score(Y_test,Y_pred)
print(f"The accuracy is {accuracy}")

"""Get the feature importance in the model's estimation and plot the most important features (TIP: To get feature importance and names you can use feature_importances_ and feature_names_in attributes of your model, respectively)"""

# Sort the feature in descending order of importance
sorted_idx = estimator.feature_importances_.argsort()
plt.barh(estimator.feature_names_in_[sorted_idx], estimator.feature_importances_[sorted_idx])
plt.xlabel("Feature Importance for model's decision")