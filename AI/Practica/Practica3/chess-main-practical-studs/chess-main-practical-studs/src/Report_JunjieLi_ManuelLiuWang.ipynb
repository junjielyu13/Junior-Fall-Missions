{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Practicals\n",
    "\n",
    "---\n",
    "\n",
    "# Practical 3: Reinforcement\tLearning\n",
    "\n",
    "## Author : Junjie Li, Manuel Liu Wang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import chess as ch\n",
    "import aichess\n",
    "import lib\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "> ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) RL\talgorithms\tmay operate\tboth\tin\tstochastic\tand\tdeterministic\tenvironments.\tThis\tfirst\t part\tof\tthe\tpractical\tconsists\tof\timplementing\ta\tQ-Leaning\talgorithm\tthat\tlearns\tto\tsolve\t the\tsame\tproblem\tproposed\tin\tpractical\t1,\twhere\tonly\tthe\twhite\tpieces\tmay\tmove. This\tis\t an\t example\t of\t the\t simpler\t case\t of\t deterministic\t environments.\t Your\t instruction\t is\t to\t implement a\tQ-learning\talgorithm,\tupdated\tvia Temporal\tDifference,\twhich\tfinds\tthe\tpath\t towards\t check-mate\t in\t the\t least\t number\t of\t moves\t possible.\t Use\t a\t table\t to\t store\t the\t corresponding\tQ-Values.\tComment\tthe\tcode\taccordingly\t(6p)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before let we set: \n",
    ">       learning rate = 0.1\n",
    ">       gamma = 0.9\n",
    ">       epsilon = 0.95\n",
    ">       episode = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] *********************************\n",
      "8 0 |   |   |   |   |   | \u001b[94mK\u001b[0m |   |   |\n",
      "7 1 |   |   |   |   |   |   |   |   |\n",
      "6 2 |   |   |   |   |   |   |   |   |\n",
      "5 3 |   |   |   |   |   |   |   |   |\n",
      "4 4 |   |   |   |   |   |   |   |   |\n",
      "3 5 |   |   |   |   |   |   |   |   |\n",
      "2 6 |   |   |   |   |   |   |   |   |\n",
      "1 7 | R |   |   |   |   | K |   |   |\n",
      "    *********************************\n",
      "      0   1   2   3   4   5   6   7   [1]\n",
      "      A   B   C   D   E   F   G   H\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "[[7, 0, 2], [7, 5, 6]] -> \n",
      "[[6, 5, 6], [7, 0, 2]] -> \n",
      "[[5, 6, 6], [7, 0, 2]] -> \n",
      "[[4, 5, 6], [7, 0, 2]] -> \n",
      "[[0, 0, 2], [4, 5, 6]] -> \n",
      "[[3, 5, 6], [0, 0, 2]] -> \n",
      "[[2, 5, 6], [0, 0, 2]] .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lib.initLogging()\n",
    "\n",
    "# intiialize board\n",
    "# current state initialization\n",
    "TA = np.zeros((8, 8))\n",
    "\n",
    "# white pieces\n",
    "TA[7][5] = 6        # white king\n",
    "TA[7][0] = 2        # white rook\n",
    "\n",
    "# black pieces\n",
    "TA[0][5] = 12       # black king\n",
    "\n",
    "# initialize board\n",
    "chess = ch.Chess(TA)\n",
    "# print board\n",
    "chess.board.print_board()\n",
    "\n",
    "# init aichess with parameters\n",
    "WhitePlayerAichess = aichess.Aichess(TA, True, True, \n",
    "                     learning_rate= 0.1, gamma = 0.9, epsilon=0.95, episode=100000)\n",
    "\n",
    "start = time.time()\n",
    "path = WhitePlayerAichess.Q_Learning()\n",
    "end = time.time()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we can run the path generated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] *********************************\n",
      "8 0 |   |   |   |   |   | \u001b[94mK\u001b[0m |   |   |\n",
      "7 1 |   |   |   |   |   |   |   |   |\n",
      "6 2 |   |   |   |   |   |   |   |   |\n",
      "5 3 |   |   |   |   |   |   |   |   |\n",
      "4 4 |   |   |   |   |   |   |   |   |\n",
      "3 5 |   |   |   |   |   |   |   |   |\n",
      "2 6 |   |   |   |   |   | K |   |   |\n",
      "1 7 | R |   |   |   |   |   |   |   |\n",
      "    *********************************\n",
      "      0   1   2   3   4   5   6   7   [1]\n",
      "      A   B   C   D   E   F   G   H\n",
      "[0] *********************************\n",
      "8 0 |   |   |   |   |   | \u001b[94mK\u001b[0m |   |   |\n",
      "7 1 |   |   |   |   |   |   |   |   |\n",
      "6 2 |   |   |   |   |   |   |   |   |\n",
      "5 3 |   |   |   |   |   |   |   |   |\n",
      "4 4 |   |   |   |   |   |   |   |   |\n",
      "3 5 |   |   |   |   |   |   | K |   |\n",
      "2 6 |   |   |   |   |   |   |   |   |\n",
      "1 7 | R |   |   |   |   |   |   |   |\n",
      "    *********************************\n",
      "      0   1   2   3   4   5   6   7   [1]\n",
      "      A   B   C   D   E   F   G   H\n",
      "[0] *********************************\n",
      "8 0 |   |   |   |   |   | \u001b[94mK\u001b[0m |   |   |\n",
      "7 1 |   |   |   |   |   |   |   |   |\n",
      "6 2 |   |   |   |   |   |   |   |   |\n",
      "5 3 |   |   |   |   |   |   |   |   |\n",
      "4 4 |   |   |   |   |   | K |   |   |\n",
      "3 5 |   |   |   |   |   |   |   |   |\n",
      "2 6 |   |   |   |   |   |   |   |   |\n",
      "1 7 | R |   |   |   |   |   |   |   |\n",
      "    *********************************\n",
      "      0   1   2   3   4   5   6   7   [1]\n",
      "      A   B   C   D   E   F   G   H\n",
      "[0] *********************************\n",
      "8 0 | R |   |   |   |   | \u001b[94mK\u001b[0m |   |   |\n",
      "7 1 |   |   |   |   |   |   |   |   |\n",
      "6 2 |   |   |   |   |   |   |   |   |\n",
      "5 3 |   |   |   |   |   |   |   |   |\n",
      "4 4 |   |   |   |   |   | K |   |   |\n",
      "3 5 |   |   |   |   |   |   |   |   |\n",
      "2 6 |   |   |   |   |   |   |   |   |\n",
      "1 7 |   |   |   |   |   |   |   |   |\n",
      "    *********************************\n",
      "      0   1   2   3   4   5   6   7   [1]\n",
      "      A   B   C   D   E   F   G   H\n",
      "[0] *********************************\n",
      "8 0 | R |   |   |   |   | \u001b[94mK\u001b[0m |   |   |\n",
      "7 1 |   |   |   |   |   |   |   |   |\n",
      "6 2 |   |   |   |   |   |   |   |   |\n",
      "5 3 |   |   |   |   |   | K |   |   |\n",
      "4 4 |   |   |   |   |   |   |   |   |\n",
      "3 5 |   |   |   |   |   |   |   |   |\n",
      "2 6 |   |   |   |   |   |   |   |   |\n",
      "1 7 |   |   |   |   |   |   |   |   |\n",
      "    *********************************\n",
      "      0   1   2   3   4   5   6   7   [1]\n",
      "      A   B   C   D   E   F   G   H\n",
      "[0] *********************************\n",
      "8 0 | R |   |   |   |   | \u001b[94mK\u001b[0m |   |   |\n",
      "7 1 |   |   |   |   |   |   |   |   |\n",
      "6 2 |   |   |   |   |   | K |   |   |\n",
      "5 3 |   |   |   |   |   |   |   |   |\n",
      "4 4 |   |   |   |   |   |   |   |   |\n",
      "3 5 |   |   |   |   |   |   |   |   |\n",
      "2 6 |   |   |   |   |   |   |   |   |\n",
      "1 7 |   |   |   |   |   |   |   |   |\n",
      "    *********************************\n",
      "      0   1   2   3   4   5   6   7   [1]\n",
      "      A   B   C   D   E   F   G   H\n"
     ]
    }
   ],
   "source": [
    "startState = path[0]\n",
    "path = path[1:]\n",
    "for nextState in path:\n",
    "    aichess.movePiece(WhitePlayerAichess, startState, nextState)\n",
    "    startState = nextState\n",
    "    WhitePlayerAichess.chess.board.print_board()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)  Using\tthe\tcode\tof\tpractical\t2\tas\ta\tstarting\tpoint,\tprogram two\tRL\tagents\t(Whites\t&\tBlacks),\t and\tmake\tthem\tlearn\tto\tcompete\teach\tother\tuntil\tcheck-mate.\tUse\tQ-Learning\ton either\t side\t(4p).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intiialize board\n",
    "# current state initialization\n",
    "TA = np.zeros((8, 8))\n",
    "\n",
    "# white pieces\n",
    "TA[7][5] = 6        # white king\n",
    "TA[7][0] = 2        # white rook\n",
    "\n",
    "# black pieces\n",
    "TA[0][5] = 12       # black king\n",
    "TA[0][0] = 8        # black rook\n",
    "\n",
    "# initialize board\n",
    "chess = ch.Chess(TA)\n",
    "# print board\n",
    "chess.board.print_board()\n",
    "\n",
    "WhitePlayerAichess = aichess.Aichess(TA, True, True, \n",
    "                     learning_rate= 0.1, gamma = 0.9, epsilon=0.95, episode=10000)\n",
    "\n",
    "BlackPlayerAichess = aichess.Aichess(TA, False, True, \n",
    "                     learning_rate= 0.1, gamma = 0.9, epsilon=0.95, episode=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning \n",
    "\n",
    "#### We don't write tie cases, and checkmate is not perfect, so it can get stuck in an infinite loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    # ----------------White Player -------------------------------- #\n",
    "    print(\"white turn\")\n",
    "    WhitePlayerCurrentState = copy.deepcopy(chess.board.currentStateW)\n",
    "    WhitePlayerAichess.chess = copy.deepcopy(chess)\n",
    "\n",
    "    WhitePlayerAichess.currentStateW = copy.deepcopy(chess.board.currentStateW)\n",
    "    WhitePlayerAichess.currentStateB = copy.deepcopy(chess.board.currentStateB)\n",
    "    WhitePlayerAichess.innitialStateW = copy.deepcopy(chess.board.currentStateW)\n",
    "    WhitePlayerAichess.innitialStateB = copy.deepcopy(chess.board.currentStateB)\n",
    "\n",
    "    WhitePlayerNextState = WhitePlayerAichess.Q_Learning()\n",
    "    nextpath = WhitePlayerNextState[1]\n",
    "    ch.movePiece(chess, WhitePlayerCurrentState, nextpath)\n",
    "\n",
    "    chess.board.print_board()\n",
    "\n",
    "    if ch.GameOver(chess.board.board):\n",
    "        print(\"Black WIN!!\")\n",
    "        break\n",
    "\n",
    "    # ----------------Black Player -------------------------------- #\n",
    "    print(\"black turn\")\n",
    "    BlackPlayerCurrentState = copy.deepcopy(chess.board.currentStateB)\n",
    "    BlackPlayerAichess.chess = copy.deepcopy(chess)\n",
    "\n",
    "    BlackPlayerAichess.currentStateW = copy.deepcopy(chess.board.currentStateW)\n",
    "    BlackPlayerAichess.currentStateB = copy.deepcopy(chess.board.currentStateB)\n",
    "    BlackPlayerAichess.innitialStateW = copy.deepcopy(chess.board.currentStateW)\n",
    "    BlackPlayerAichess.innitialStateB = copy.deepcopy(chess.board.currentStateB)\n",
    "\n",
    "    BlackPlayerNextState = BlackPlayerAichess.Q_Learning()\n",
    "    nextpath = BlackPlayerNextState[1]\n",
    "    ch.movePiece(chess, BlackPlayerCurrentState, nextpath)\n",
    "\n",
    "    chess.board.print_board()\n",
    "\n",
    "    if ch.GameOver(chess.board.board):\n",
    "        print(\"Black WIN!!\")\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Which\tdifferences\tcan\tyou\tdescribe\tof\tthe\tbehaviour of\tthese\tagents\twith\trespect\t\n",
    "to\tthat\tof\tpoint\t1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, we come to the confrontation game of two aichess. A big difference from the first question is that now the game is for each aichess to take the next step, so it is not enough to blindly find the optimal solution, because the aichess cannot go all the way to the end, so we can train ai to obtain the global optimal solution next step in . Then update the chess table to let aichess retrain the current situation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) How\tlong\tdoes\tit\ttake\tto\tlearn?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Their learning time depends on their learning rate, gamma, epsilon, and episode values. If the learning rate is too small, it will greatly reduce the convergence speed and increase the training time. Different gamma and epsilon values will also have a certain degree of influence on whether it can be fast The optimal solution is obtained, and the episode represents the number of training times. A reasonable episode value can often find the best performance in the training time and the optimal solution. Their learning time depends on their learning rate, "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What\thappens\tif\tyou\tvary\ttheir\trelative\tlearning\trates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WhitePlayerAichess = aichess.Aichess(TA, True, True, \n",
    "                     learning_rate= 0.05, gamma = 0.9, epsilon=0.95, episode=10000)\n",
    "\n",
    "BlackPlayerAichess = aichess.Aichess(TA, False, True, \n",
    "                     learning_rate= 0.2, gamma = 0.9, epsilon=0.95, episode=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **Learning rate** is an important hyperparameter in supervised learning and Q-learning, \n",
    "which determines whether the objective function can converge to the local minimum and when it can converge \n",
    "to the minimum. An appropriate learning rate can make the objective function converge to a local minimum\n",
    "within an appropriate time.\n",
    "\n",
    ">When the learning rate is set too small, the convergence process will become very slow. And when the learning \n",
    "rate is set too large, the gradient may oscillate back and forth near the minimum value, and may even fail \n",
    "to converge.\n",
    "\n",
    "> In this example, if we set different learning rates for the two aichesses, if the learning rate is too small for one of them, it will greatly reduce the convergence speed and increase the training time. If one of them sets the learning rate too large, it may cause oscillations back and forth on both sides of the optimal solution, so that the optimal solution cannot be found."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "> ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c565597e682a3ecd56ca2f3628507b813298df9842f4b1af2ebb7ab3d7b1034d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
