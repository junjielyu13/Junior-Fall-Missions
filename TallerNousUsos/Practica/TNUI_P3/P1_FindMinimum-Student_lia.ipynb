{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimització - Descens del gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’optimització matemàtica o la programació matemàtica és la selecció d’un millor element a partir d’un conjunt d’alternatives disponibles. Problemes d’optimització de tipus sorgeixen en totes les disciplines quantitatives, des de la informàtica i l’enginyeria fins a la investigació d’operacions i l’economia, i el desenvolupament de mètodes de solució ha estat d’interès en les matemàtiques durant segles.\n",
    "\n",
    "En el cas més senzill, un problema d’optimització consisteix a maximitzar o minimitzar una funció real mitjançant l’elecció sistemàtica dels valors d’entrada dins d’un conjunt permès i el càlcul del valor de la funció. \n",
    "La generalització de la teoria i tècniques d’optimització a altres formulacions constitueix una àmplia àrea de matemàtiques aplicades. \n",
    "Més generalment, l’optimització inclou trobar valors \"els millors disponibles\" d'alguna funció objectiva donat un domini (o entrada) definits, incloent una varietat de diferents tipus de funcions objectives i diferents tipus de dominis.\n",
    "\n",
    "\n",
    "Es pot representar un problema d'optimització de la següent manera:\n",
    "\n",
    "- Tenint en compte: una funció $f : A \\to \\mathbb{R}$ des d'algun conjunt $A$ als nombres reals.\n",
    "- Buscat: un element ${x}_0 \\in A$ tal que $f({x}_{0}) \\leq f({x})$ per a tots els ${x} \\in A$ (\"minimització\") o tal que $f({x}_{0}) \\geq f({x})$  per a tots els ${x} \\in A$ (\"maximització\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La metodologia crítica per resoldre problemes d'optimització és el **descens del gradient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior:**\n",
    "S'anomena *derivada parcial* d'una funció de diverses variables a la seva derivada respecte a una d'aquestes variables, deixant les altres constants. Cada derivada indica com varia $f$ quan fem un petit canvi en la variable corresponent. \n",
    "\n",
    "El *gradient* de la funció $f(x_1, \\cdots, x_n)$ és un vector format per les derivades parcials de la funció. \n",
    "Aquest dóna la direcció en la qual la funció creix més ràpidament. \n",
    "\n",
    "$$\\nabla {f} = (\\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cas Unidimensionals\n",
    "\n",
    "Suposem que volem calcular el nimim numèric de la següent funció unidimensional:\n",
    "\n",
    "$$ f_1(x) = x^2 $$\n",
    "\n",
    "La derivada parcial de la funcio $f$ és:\n",
    "\n",
    "$$ \\frac{\\partial f_1}{\\partial x} = 2 x$$\n",
    "\n",
    "El primer pas que farem és visualitzar la funció i veure el comportament del gradient / derivada sobre un conjunt de punts que pertanyen el domini de la funció."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return x**2\n",
    "\n",
    "def df1(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot\n",
    "X = np.arange(-5, 5.01, 0.25)\n",
    "fig = plt.figure()\n",
    "plot_1d(X, f1(X), fig)\n",
    "plot_gradient (X, f1(X), df1(X), fig=fig)\n",
    "fig.gca().set_xlim(-10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per tant, una possible estratègia per maximitzar (*minimitzar*) la funció és reptir la següent estratègia:\n",
    "\n",
    "1. Col·locar-se en un punt aleatori (un valor aleatori de $\\mathbf{x}$)\n",
    "2. Calcular el vector gradient.\n",
    "3. Fer un petit pas en la direcció (*contraria*) del vector gradient: $x^{k+1} = x^{k}-\\alpha^{k} \\nabla f(x^{k})$\n",
    "4. Repetir des de 2\n",
    "\n",
    "La cerca s'acabaria quan el vector gradient és zero, que vol dir que hem trobat un màxim (*mínim*). Donat que la presició flotant potser evita que arribem mai a aquest nombre, afegim dos mètodes més per acabar la cerca:\n",
    "\n",
    "1. Si el moviment que realitzem és més petit que un epsilon ($|x^{k+1} - x^{k}| < eps$)\n",
    "2. Si realitzem més de `max_iters` iteracions\n",
    "\n",
    "Si la funció és **unimodal** (conté un únic màxim (*mínim*)), hem trobat una la nostra solució. \n",
    "Sinó, pot ser que estem en un màxim (*mínim*) local i que sigui una solució no satisfactòria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementa la funció *gradient_descend_1d* seguint el procediment proposat. Durant el procés d'optimització visualitza els punts de la gràfica per on vagis passant.**\n",
    "\n",
    " > Recordeu que els ordinadors generen un error numèric, per tant no heu d'igualar mai a zero, heu de fer servir una tolerància.\n",
    "Definiu també un nombre màxim d'iteracions que l'algorisme pot realitzar per arribar al punt desitjat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descend_1d(grad, x, alpha, eps=1e-3, max_iters=1e2):\n",
    "    \"\"\"\n",
    "    Aquesta funció implementa l'algorisme de descens pel gradient, és a dir,\n",
    "    donat un punt inicial, el gradient i el pas, intenta trobar el mínim\n",
    "    de la funció seguint el gradient en direcció oposada.\n",
    "    \n",
    "    :param grad: Gradient de la funció\n",
    "    :param x: Punt inicial\n",
    "    :param alpha: Pas de cada iteració\n",
    "    :param eps: Moviment mínim realitzat abans de parar\n",
    "    :param max_epochs: Iteracions màximes a realitzar\n",
    "    :return: La funció retornarà una llista/tupla amb:\n",
    "        * Una np.array [X] amb el punt inicial més els punts on s'ha mogut a cada iteració, \n",
    "            on X és el nombre d'iteracions fetes + 1\n",
    "        * Una np.array [2] amb els punts del recorregut\n",
    "    \"\"\"\n",
    "    # AQUÍ EL TEU CODI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-3, 3.01, 0.25)\n",
    "points, minimum = gradient_descend_1d(df1, 2, .1)\n",
    "\n",
    "fig = plt.figure()\n",
    "plot_gradient_descend_1d(f1, X, points, minimum, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Però... són totes els funcions unimodals? Obviament no.\n",
    "\n",
    "Podem tenir funcions que contenen un màxim i un mínim. Per veure com es comporten aquestes funcions **repeteix el procediment anterior amb aquesta nova funció**.\n",
    "\n",
    "$$f_2(x) = x^3 - 2x + 2$$\n",
    "\n",
    "$$ \\frac{\\partial f_2}{\\partial x} = 3x^2 -2 x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2 (x):\n",
    "    return x**3 - 2*x + 2\n",
    "\n",
    "def df2(x):\n",
    "    return 3*x**2 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-2, 2, 0.1)\n",
    "fig = plt.figure()\n",
    "plot_1d(X, f2(X), fig)\n",
    "plot_gradient(X, f2(X), df2(X), fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Començant en el punt $x=0.75$, és possible arribar a valors de $x < -1$ en 10 o menys iteracions? Fer proves demostrant que es pot, o no, arribar-hi.**\n",
    "\n",
    "**Justifica el comportament del procés d'optimització**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "* RECORDA JUSTIFICAR LA RESPOSTA\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-20, 20, 0.1)\n",
    "x0 = 0.75\n",
    "points, minimum = gradient_descend_1d(df2, x0, 1, max_iters=10)\n",
    "\n",
    "fig = plt.figure()\n",
    "plot_gradient_descend_1d(f2, X, points, minimum, fig)\n",
    "fig.gca().set_ylim([-2, 5])\n",
    "fig.gca().set_xlim([-2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podem assegurar que sempre arribem a un minim?\n",
    "\n",
    "Suposa ara la funció\n",
    "$$\n",
    "f_{2.2}(x) = \\frac{x}{1+x^2}\n",
    "$$\n",
    "amb gradient\n",
    "$$ \\frac{\\partial f_{2.2}}{\\partial x} = \\frac{1-x^2}{(1+x^2)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_2(x):\n",
    "    return x / (1 + x**2)\n",
    "\n",
    "def df2_2(x):\n",
    "    return (1 - x**2) / (1 + x**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-60, 60, 0.1)\n",
    "fig = plt.figure()\n",
    "plot_1d(X, f2_2(X), fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = (np.random.random() - 0.5) * 10\n",
    "points, minimum = gradient_descend_1d(df2_2, x0, 1)\n",
    "\n",
    "fig = plt.figure()\n",
    "plot_gradient_descend_1d(f2_2, X, points, minimum, fig)\n",
    "fig.gca().set_xlim([-10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justifica el comportament per diferent punts d'inici. Podem arribar sempre al mínim?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "* RECORDA JUSTIFICAR LA RESPOSTA\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas N-dimensionals\n",
    "\n",
    "Podem aplicar el procediment anterior quan tenim funcions dos-dimensionals? La resposta és que sí. \n",
    "\n",
    "L'unic canvi en el procediment és que ara caldrà avançar en dues direccions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigui la funció 2-dimensional $f_3$:\n",
    "$$ f_3(x_1, x_2) = x_1^2 + x_2^2$$\n",
    "\n",
    "i el seu gradient:\n",
    "\n",
    "$$\\nabla {f_3(x_1, x_2)} = \n",
    "\\left( \\frac{\\partial f_3}{\\partial x_1}, \\frac{\\partial f_3}{\\partial x_2}\\right) = \n",
    "\\left( 2 x_1, 2 x_2 \\right)$$\n",
    "\n",
    "Visualitza la forma de grafica i el comportament del seu gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3 ( x, y ):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def grad_f3(x, y):\n",
    "    return np.asarray((2 * x, 2 * y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface to plot\n",
    "fig = plt.figure()\n",
    "X, Y, Z = generate_grid(f3)\n",
    "plot_2d(X, Y, Z, fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per veure millor el gradient en aquest cas, podem imprimir el seu comportament en un pla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "project_gradient(f3, grad_f3, fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implamenteu l'algorisme anterior amb el cas 2-dimensional.\n",
    "\n",
    "Recordeu amb amb la llibreria numpy podeu sumar o restar vectors, multiplicar per un escalar, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descend_2d(gradient, x, alpha=0.1, eps=1e-6, max_iter=1000, verbose=True):\n",
    "    \"\"\"\n",
    "    Aquesta funció implementa l'algorisme de descens pel gradient en el \n",
    "    cas 2-DIMENSIONAL, és a dir, donat un punt inicial (x_0, x_1), \n",
    "    el gradient i el pas, intenta trobar el mínim de la funció seguint \n",
    "    el gradient en direcció oposada.\n",
    "    \n",
    "    :param grad: Gradient de la funció\n",
    "    :param x: Punt inicial\n",
    "    :param alpha: Pas de cada iteració\n",
    "    :param eps: Moviment mínim realitzat abans de parar\n",
    "    :param max_iter: Iteracions màximes a realitzar\n",
    "    :param verbose: En case de ser True, la funció ha d'imprimir el nombre d'iteracions fetes\n",
    "        abans de retornar\n",
    "    :return: La funció retornarà una llista/tupla amb:\n",
    "        * Una np.array [X, 2] amb el punt inicial més els punts on s'ha mogut a cada iteració, \n",
    "            on X és el nombre d'iteracions fetes + 1\n",
    "        * Una np.array [2] amb el punts dels recoregut\n",
    "    \"\"\"\n",
    "    # AQUÍ EL TEU CODI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.asarray([1, 1])\n",
    "points, minimum = gradient_descend_2d(grad_f3, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Minimum: \", np.round(minimum,3))\n",
    "\n",
    "fig = plt.figure()\n",
    "X, Y, _ = generate_grid(f3)\n",
    "plot_gradient_descend_2d(f3, grad_f3, X, Y, points, minimum, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considera ara la funció\n",
    "$$f_4(x_1, x_2) = 100(x_1^2 + x_2^2)$$\n",
    "\n",
    "amb gradient\n",
    "\n",
    "$$\\nabla {f_4(x_1, x_2)} = \n",
    "\\left( \\frac{\\partial f_4}{\\partial x_1}, \\frac{\\partial f_4}{\\partial x_2}\\right) = \n",
    "\\left( 200 x_1, 200 x_2 \\right)$$\n",
    "\n",
    "En aquest cas, igual que en molts altres, és més útil utilizar el gradient normalizat per aplicar l'algorisme. \n",
    "\n",
    "**Fent servir `gradient_descend_2d`, podeu trobar el mínim de les funcions $f_3$ i $f_4$ amb el següents paràmetres:**\n",
    "\n",
    "* $\\alpha$ igual en les dues crides, es pot modificar\n",
    "* $x=(1, 1)$ en les dues crides\n",
    "* `max_iter=100` en les dues crides\n",
    "* `eps=1e-6` en les dues crides\n",
    "\n",
    "**Justifica perque són capaços, o no, de trobar el mínim amb aquest paràmetres. Quines dificultats presenta?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "* RECORDA JUSTIFICAR LA RESPOSTA\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(x, y):\n",
    "    return 100 * (x**2 + y**2)\n",
    "\n",
    "def grad_f4(x, y):\n",
    "    return np.asarray((200 * x, 200 * y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.asarray((1, 1))\n",
    "alpha = 0.01\n",
    "points_f3, minimum_f3 = gradient_descend_2d(grad_f3, x0, alpha=alpha, max_iter=100, eps=1e-6)\n",
    "points_f4, minimum_f4 = gradient_descend_2d(grad_f4, x0, alpha=alpha, max_iter=100, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "X, Y, _ = generate_grid(f3)\n",
    "plot_gradient_descend_2d(f3, grad_f3, X, Y, points_f3, minimum_f3, fig)\n",
    "print(minimum_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "X, Y, _ = generate_grid(f4)\n",
    "plot_gradient_descend_2d(f4, grad_f4, X, Y, points_f4, minimum_f4, fig)\n",
    "print(minimum_f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temps de convergència\n",
    "\n",
    "Observa ara la següent funció, anomenada funció de **Rosenbrock**.\n",
    "\n",
    "$$ f(x_1, x_2) = (a - x_1)^2 + b (x_2 - x_1^2)^2$$\n",
    "\n",
    "**El mínim d'aquesta funció es troba al punt: $(x_1, x_2) = (a, a^{2})$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ROS = 1\n",
    "\n",
    "def f_ros(x, y, a=A_ROS, b=10):\n",
    "    return (a-x)**2 + b*(y-x**2)**2\n",
    "\n",
    "def grad_f_ros(x, y, a=A_ROS, b=10):\n",
    "    return np.asarray((-2*a + 4*b*x**3 - 4*b*x*y + 2*x, 2*b*(y-x**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface to plot\n",
    "fig = plt.figure()\n",
    "X, Y, Z = generate_grid(f_ros)\n",
    "plot_2d(X, Y, Z, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "project_gradient(f_ros, grad_f_ros, fig)\n",
    "plt.scatter(A_ROS, A_ROS**2, marker='x', color='r', s=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquesta funció té un mínim molt pla, la pendent és pràcticament nula al sel voltat, pel que els mètodes iteratius tarden molt a trobar el mínim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "x0 = np.asarray((-1.5, 1.5))\n",
    "points, minimum = gradient_descend_2d(grad_f_ros, x0, alpha=0.01, max_iter=10000)\n",
    "X, Y, _ = generate_grid(f_ros)\n",
    "plot_gradient_descend_2d(f_ros, grad_f_ros, X, Y, points, minimum, fig)\n",
    "print(minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trobant el pas òptim\n",
    "\n",
    "Una possibilitat per fer menys iteracions i accelerar la convergència és trobant el valor de $\\alpha$ automàticament a cada pas, és a dir aquella $\\alpha$ que fa mínim el valor de $f$.\n",
    "\n",
    "L'algorisme és tal que:\n",
    "\n",
    "1. Col·locar-se en un punt aleatori (un valor aleatori de $\\mathbf{x}$)\n",
    "2. Calcular el vector gradient\n",
    "3. $\\alpha = 10$\n",
    "4. Calcular $x^{k+1} = x^{k}-\\alpha \\nabla f(x^{k})$\n",
    "5. Si $f(x^{k+1}) > f(x^{k})$, $\\alpha = \\alpha / 2$, descartar $x^{k+1}$ i tornar al pas 4\n",
    "6. En cas contrari, moure's a $x^{k+1}$ i repetir des de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descend_2d_auto(f, gradient, x, eps=1e-6, max_iter=1000, initial_alpha=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Aquesta funció implementa l'algorisme de descens pel gradient en el \n",
    "    cas 2-DIMENSIONAL, és a dir, donat un punt inicial (x_0, x_1), \n",
    "    el gradient i el pas, intenta trobar el mínim de la funció seguint \n",
    "    el gradient en direcció oposada.\n",
    "    \n",
    "    :param f: Funció a minimitzar\n",
    "    :param grad: Gradient de la funció\n",
    "    :param x: Punt inicial\n",
    "    :param eps: Moviment mínim realitzat abans de parar\n",
    "    :param max_iter: Iteracions màximes a realitzar\n",
    "    :param initial_alpha: Pas inicial a cada iteració, corresponent al punt 3 anterior\n",
    "    :param verbose: En case de ser True, la funció ha d'imprimir el nombre d'iteracions fetes\n",
    "        abans de retornar\n",
    "    :return: La funció retornarà una llista/tupla amb:\n",
    "        * Una np.array [X, 2] amb el punt inicial més els punts on s'ha mogut a cada iteració, \n",
    "            on X és el nombre d'iteracions fetes + 1\n",
    "        * Una np.array [2] amb el punts dels recoregut\n",
    "    \"\"\"\n",
    "    # AQUÍ EL TEU CODI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "x0 = np.asarray((-1.5, 1.5))\n",
    "points, minimum = gradient_descend_2d_auto(f_ros, grad_f_ros, x0, max_iter=10000)\n",
    "X, Y, _ = generate_grid(f_ros)\n",
    "plot_gradient_descend_2d(f_ros, grad_f_ros, X, Y, points, minimum, fig)\n",
    "print(minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gradient_descend_2d(grad_f_ros, x0, alpha=0.01, max_iter=10000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gradient_descend_2d_auto(f_ros, grad_f_ros, x0, max_iter=10000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c565597e682a3ecd56ca2f3628507b813298df9842f4b1af2ebb7ab3d7b1034d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
