{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision - P4\n",
    "\n",
    "### **Carefully read the file `README.md` as well as the following instructions before start coding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delivery\n",
    "\n",
    "Up to **1 point out of 10** will be penalized if the following requirements are not fulfilled:\n",
    "\n",
    "- Implemented code should be commented.\n",
    "\n",
    "- The questions introduced in the exercises must be answered.\n",
    "\n",
    "- Add title to the figures to explain what is displayed.\n",
    "\n",
    "- Comments and answers need to be in **English**.\n",
    "\n",
    "- The deliverable must be a file named **P4_Student1_Student2.zip** that includes:\n",
    "    - The notebook P4_Student1_Student2.ipynb completed with the solutions to the exercises and their corresponding comments.\n",
    "\n",
    "**Deadline (Campus Virtual): November 22th, 23:00 h** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "## Descriptors extraction for object detection, based on template matching, ORB, and HOG\n",
    "==============================================================================================\n",
    "\n",
    "The main topics of Laboratory 4 are:\n",
    "\n",
    "    4.1) SSD and Normalized Cross-correlation for template matching\n",
    "\n",
    "    4.2) HOG image descriptor for object (person) detection\n",
    "\n",
    "    4.3) Recognition by correspondance, based on feature extraction (ORB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete this practicum, the following concepts need to be understood: template matching, feature localization (Harris, Censure), feature descriptor (HOG,ORB, Sift) methods.\n",
    "\n",
    "It is highly recommendable to structure the code in functions in order to reuse code for different tests and images and make it shorter and more readable. Specially the visualization commands should be encapsulated in separate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Template matching\n",
    "\n",
    "**1.1** Given the image 'einstein.png' and the template image 'eye.png', detect the location of the template in the image comparing the use of:\n",
    "- SSD distance (hint: norm() in numpy.linalg).\n",
    "- normalized cross-correlation (hint: match_template() of skimage.feature).\n",
    "\n",
    "Don't forget to normalize the images (having pixel values between [0,1]) before comparing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for template matching applying SSD distance and normalized cross-correlation and display the results in the following format:\n",
    "    \n",
    "<img src=\"images_notebook/tm.png\" width=\"400\" height=\"120\">\n",
    "\n",
    "**Use titles in all figures to understand what is being displayed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** How does the result of the template matching change if the image changes its contrast (for example if you make it clearer or darker)? \n",
    "\n",
    "Similarly to the previous case, please, visualize the euclidean distance and normalized cross-correlation images as well as the binarized (thresholded) images in the two cases.\n",
    "\n",
    "**Note:** Use titles of the figures to explain what is displayed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read `einstein_br.png` and display the results on this image, using `eye.png` as template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the minimum SSD and the maximum Normalized Cross-Correlation (NCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3** Read `einstein_mask.png` and display the results on this image, using the `eye.png` template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the minimum SSD and the maximum NCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the distances between the template and the image around the eyes of the image? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment the template matching process:\n",
    "* Is the algorithms affected by contrast changes in the image?\n",
    "* How do metrics (i.e. minimum euclidean distance and maximum NCC) change in all previous cases? Is there a big difference among these values?\n",
    "* What parameters it has and which measure for image comparisons works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4** How does the result of the template matching change if instead the template is the one that changes its contrast (for example if you make it clearer or darker)? To this purpose, use the `eye_br.png` template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how the result changes if the template is rotated.\n",
    "\n",
    "Visualize the template and its rotation by 2º, 5º, 10º, 15º and 20º. Obtain again the template matching using the Euclidean distance and normalized cross-correlation.\n",
    "\n",
    "**Help:** use the function rotate() in skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment the template matching process:\n",
    "* Please, explain briefly the algorithm, including advantages and disadvantage\n",
    "* Is the algorithms affected by contrast changes in the tempate image?\n",
    "* What parameters it has and which measure for image comparisons works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Histogram of Oriented Gradients\n",
    "\n",
    "In this section we will treat the following topic: HOG image descriptor. Application to person detection.\n",
    "\n",
    "The Histogram of Oriented Gradients (HOG) feature descriptor is classical image descriptor for object detection.\n",
    "\n",
    "Given the image `person_template.bmp` and the folder `/images/TestPersonImages/`, apply the HOG descriptor in order to detect where there is a person in the images. To this purpose, apply the \"sliding window\" technique. We use images from GRAZ 01 data from [INRIA datasets](http://pascal.inrialpes.fr/data/human/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.** Read the template (`person_template.bmp`), obtain its HOG descriptor (with the optimal parameters) and visualize it.\n",
    "\n",
    "Help: the HOG detector function is in the skimage.feature library ([Help](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html#sphx-glr-auto-examples-features-detection-plot-hog-py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**2.2.** Apply the HOG descriptor on the complete set of images for person detection.\n",
    "\n",
    "a) Read images from the folder \"TestPersonImages\", slide a window on each image, obtain the HOG descriptor and compare to the HOG descriptor of the person template. \n",
    "\n",
    "b) Visualize the location in the image that is the most similar to the person template using the distance between the template and test image descriptors.\n",
    "\n",
    "Display the results of every person detection following this format:\n",
    "\n",
    "<img src=\"images_notebook/hog.png\" width=\"800\" height=\"100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the dimension of your HOG descriptor? Explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count on how many images were the persons detected correctly and discuss the failures.\n",
    "What do you think can be the reasons for the failures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test several values of the parameters (``orientations``, ``pixels_per_cell``, ``cells_per_block``) to show which are the optimal values for the person detection problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment the object detection process:\n",
    "* Please, explain briefly the algorithm, including advantages and disadvantage\n",
    "* Do you see any advantages of the HOG-based object detector compared to the template-based object detection? (The answer should be up to 10-15 lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 ORB feature detector and binary descriptor\n",
    "\n",
    "Let us consider the problem of feature extraction that contains two subproblems: \n",
    "- feature location, \n",
    "- image feature description.\n",
    "\n",
    "Let us focus on ORB, an approximation of SIFT method, and analyse if ORB is  scale and rotation invariant, a property that is very important for real-time applications.\n",
    "\n",
    "**Hint:** `ORB` is a function within the module `skimage.feature`\n",
    "                             \n",
    "**Help**: We suggest to have a look at the [ORB example](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_orb.html) how to compute the ORB descriptors and find the descriptors match. You can use the function match_descriptors from `skimage.feature` module in order to compute and show the similar detected descriptors of the given images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Detect the censure in the image `starbucks4.jpg`. Analyze and discuss the effect of different values of the parameters in censure function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Detect the correspondences between the model image `starbucks.jpg` with the scene image `starbucks4.jpg`. You can adapt the code from the [ORB example](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_orb.html) above. \n",
    "\n",
    "Define a function get_ORB implementing the algorithm in order to be able to apply it on different images. Comment the code in detail.\n",
    "\n",
    "**Hint: If the function plot_matches() gives you an error you can use the plot_matches_aux() at the end of this file.**\n",
    "\n",
    "Analyze and discuss the effect of different values of the parameter `max_ratio` in the match_descriptors function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeate the experiment comparing the `starbucks.jpg` image as a model, and showing its matches to all Starbucks images, sorting them based on their similarity to the model. Comment when does the algorithm work better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Repeate the experiment: \n",
    "- Changing the orientation of the model image by rotating it and comparing it with its original version. Help: you can use the rotate() function from skimage.transform \n",
    "- Change the scale and orientation of the scene image and compare it with the model image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Help:** To do so, you can use the function given below as example:\n",
    "\n",
    "```\n",
    "import transform as tf\n",
    "rotationdegrees = 180\n",
    "img_rotated = tf.rotate(image2transform, rotationdegrees)\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "tform = tf.AffineTransform(scale=(1.2, 1.2), translation=(0, -100))\n",
    "img_transformed = tf.warp(image2transform, tform)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional)** Repeat the experiment (3.1 to 3.3) with a new group of images. You could use Coca-Cola advertisements or from another famous brand, easily to find on internet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** Analysis of the applied techniques and results\n",
    "\n",
    "- What are the advantages of the ORB object detection with respect to the HOG and template object detector?\n",
    "\n",
    "- What would happen if you analyse an image that does not contain the Starbucks logo? \n",
    "\n",
    "- Could you think of ways of defining a quality measure for the correspondance between two images? (no need of implementing it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the plot_matches() function gives you some problems, you can use the following one:\n",
    "\n",
    "from skimage.util import img_as_float\n",
    "import numpy as np\n",
    "\n",
    "def plot_matches_aux(ax, image1, image2, keypoints1, keypoints2, matches,\n",
    "                 keypoints_color='k', matches_color=None, only_matches=False):\n",
    "    \"\"\"Plot matched features.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        Matches and image are drawn in this ax.\n",
    "    image1 : (N, M [, 3]) array\n",
    "        First grayscale or color image.\n",
    "    image2 : (N, M [, 3]) array\n",
    "        Second grayscale or color image.\n",
    "    keypoints1 : (K1, 2) array\n",
    "        First keypoint coordinates as ``(row, col)``.\n",
    "    keypoints2 : (K2, 2) array\n",
    "        Second keypoint coordinates as ``(row, col)``.\n",
    "    matches : (Q, 2) array\n",
    "        Indices of corresponding matches in first and second set of\n",
    "        descriptors, where ``matches[:, 0]`` denote the indices in the first\n",
    "        and ``matches[:, 1]`` the indices in the second set of descriptors.\n",
    "    keypoints_color : matplotlib color, optional\n",
    "        Color for keypoint locations.\n",
    "    matches_color : matplotlib color, optional\n",
    "        Color for lines which connect keypoint matches. By default the\n",
    "        color is chosen randomly.\n",
    "    only_matches : bool, optional\n",
    "        Whether to only plot matches and not plot the keypoint locations.\n",
    "    \"\"\"\n",
    "\n",
    "    image1 = img_as_float(image1)\n",
    "    image2 = img_as_float(image2)\n",
    "\n",
    "    new_shape1 = list(image1.shape)\n",
    "    new_shape2 = list(image2.shape)\n",
    "\n",
    "    if image1.shape[0] < image2.shape[0]:\n",
    "        new_shape1[0] = image2.shape[0]\n",
    "    elif image1.shape[0] > image2.shape[0]:\n",
    "        new_shape2[0] = image1.shape[0]\n",
    "\n",
    "    if image1.shape[1] < image2.shape[1]:\n",
    "        new_shape1[1] = image2.shape[1]\n",
    "    elif image1.shape[1] > image2.shape[1]:\n",
    "        new_shape2[1] = image1.shape[1]\n",
    "\n",
    "    if new_shape1 != image1.shape:\n",
    "        new_image1 = np.zeros(new_shape1, dtype=image1.dtype)\n",
    "        new_image1[:image1.shape[0], :image1.shape[1]] = image1\n",
    "        image1 = new_image1\n",
    "\n",
    "    if new_shape2 != image2.shape:\n",
    "        new_image2 = np.zeros(new_shape2, dtype=image2.dtype)\n",
    "        new_image2[:image2.shape[0], :image2.shape[1]] = image2\n",
    "        image2 = new_image2\n",
    "\n",
    "    image = np.concatenate([image1, image2], axis=1)\n",
    "\n",
    "    offset = image1.shape\n",
    "\n",
    "    if not only_matches:\n",
    "        ax.scatter(keypoints1[:, 1], keypoints1[:, 0],\n",
    "                   facecolors='none', edgecolors=keypoints_color)\n",
    "        ax.scatter(keypoints2[:, 1] + offset[1], keypoints2[:, 0],\n",
    "                   facecolors='none', edgecolors=keypoints_color)\n",
    "\n",
    "    ax.imshow(image, interpolation='nearest', cmap='gray')\n",
    "    ax.axis((0, 2 * offset[1], offset[0], 0))\n",
    "\n",
    "    for i in range(matches.shape[0]):\n",
    "        idx1 = matches[i, 0]\n",
    "        idx2 = matches[i, 1]\n",
    "\n",
    "        if matches_color is None:\n",
    "            color = np.random.rand(3)\n",
    "        else:\n",
    "            color = matches_color\n",
    "\n",
    "        ax.plot((keypoints1[idx1, 1], keypoints2[idx2, 1] + offset[1]),\n",
    "                (keypoints1[idx1, 0], keypoints2[idx2, 0]),\n",
    "                '-', color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c565597e682a3ecd56ca2f3628507b813298df9842f4b1af2ebb7ab3d7b1034d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
